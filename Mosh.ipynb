{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Solo7602/MADPA/blob/main/Mosh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "ePm5kER4MqDu",
        "outputId": "5bedf138-d784-42b7-ba6d-d3240caaf486"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'indexer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-25b3c182b746>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mautocorrect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpeller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spellchecker/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mspellchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellchecker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spellchecker/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionaryIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_detect_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'indexer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"  # избегаем конфликта с TensorFlow\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "from razdel import tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from autocorrect import Speller\n",
        "from tqdm import tqdm\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Инициализация проверяющего (русский язык)\n",
        "spell = Speller(lang='ru')\n",
        "\n",
        "# Загрузка сленгового словаря\n",
        "with open('russian_slang_words.json', 'r', encoding='utf-8') as f:\n",
        "    slang_words = set(json.load(f))\n",
        "\n",
        "# Загружаем данные\n",
        "data = pd.read_csv('10k_dataset_processed_final.csv').head(40000)\n",
        "data['Comment'] = data['Comment'].fillna(\"\")\n",
        "\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "patterns = \"[0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
        "\n",
        "russian_stopwords = stopwords.words('russian')\n",
        "english_stopwords = stopwords.words('english')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(patterns, ' ', text)\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    normalized_words = []\n",
        "    for word in words:\n",
        "        if word in russian_stopwords or word in english_stopwords or len(word) <= 2:\n",
        "            continue\n",
        "        if word.isalpha():\n",
        "            if any('\\u0400' <= char <= '\\u04FF' for char in word):\n",
        "                normalized_words.append(morph.parse(word)[0].normal_form)\n",
        "            else:\n",
        "                normalized_words.append(word)\n",
        "\n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "def preprocess_text_tokens(text):\n",
        "    text = re.sub(patterns, ' ', text)\n",
        "    text = text.lower()\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    normalized_words = []\n",
        "    for word in words:\n",
        "        if len(word) <= 2:\n",
        "            continue\n",
        "        if word in russian_stopwords or word in english_stopwords:\n",
        "            continue\n",
        "        if word.isalpha():\n",
        "            if any('\\u0400' <= char <= '\\u04FF' for char in word):\n",
        "                lemma = morph.parse(word)[0].normal_form\n",
        "                normalized_words.append(lemma)\n",
        "            else:\n",
        "                normalized_words.append(word)\n",
        "\n",
        "    return normalized_words\n",
        "\n",
        "def analyze_comment(text):\n",
        "    tokens = preprocess_text_tokens(text)\n",
        "    normalized_text = ' '.join(tokens)\n",
        "\n",
        "    # Проверка орфографии\n",
        "    corrected_tokens = [spell(word) for word in tokens]\n",
        "    misspelled = [original for original, corrected in zip(tokens, corrected_tokens) if original != corrected]\n",
        "\n",
        "    num_errors = len(misspelled)\n",
        "    has_errors = num_errors > 0\n",
        "\n",
        "    # Проверка на наличие сленга\n",
        "    slang_found = [word for word in tokens if word in slang_words]\n",
        "    num_slang = len(slang_found)\n",
        "    has_slang = num_slang > 0\n",
        "\n",
        "    return pd.Series([num_errors, has_errors, num_slang, has_slang])\n",
        "\n",
        "data['Comment_processed'] = data['Comment'].apply(preprocess_text)\n",
        "data['Comment_processed'] = data['Comment_processed'].fillna(\"\")\n",
        "\n",
        "# Применение анализа к каждому комментарию\n",
        "data[['Spell_Errors', 'Has_Spell_Errors', 'Slang_Count', 'Has_Slang']] = data['Comment'].apply(analyze_comment)\n",
        "\n",
        "# # Создание возрастных групп\n",
        "bins = [0, 30, 45, 100]\n",
        "labels = [0, 1, 2]\n",
        "data['Age_Group'] = pd.cut(data['Age'], bins=bins, labels=labels).astype(int)\n",
        "\n",
        "# # Числовые признаки\n",
        "def count_punctuation(text):\n",
        "    return len(re.findall(r\"[.,!?;:\\-–—]\", text))\n",
        "\n",
        "def ends_with_punctuation(text):\n",
        "    text = text.strip()\n",
        "    return int(text[-1] in {'.', ',', '!', '?'}) if text else 0\n",
        "\n",
        "data['Emoji_Count'] = data['Comment'].apply(lambda x: len(re.findall(r'[\\U0001F600-\\U0001F64F]', x)))\n",
        "data['Punctuation_Count'] = data['Comment'].apply(count_punctuation)\n",
        "data['Word_Count'] = data['Comment'].apply(lambda x: len(x.split()))\n",
        "data['Avg_Word_Length'] = data['Comment'].apply(lambda x: np.mean([len(w) for w in x.split()]) if x.split() else 0)\n",
        "data['Ends_With_Punct'] = data['Comment'].apply(ends_with_punctuation)\n",
        "data['Comment_Length'] = data['Comment'].apply(len)\n",
        "\n",
        "# Сохраните обновленный DataFrame в файле\n",
        "data.to_csv('40000_dataset_processed.csv', index=False)\n",
        "\n",
        "\n",
        "# numeric_features = ['emoji_count', 'punctuation_count', 'word_count', 'avg_word_length', 'ends_with_punct', 'Comment_Length']\n",
        "# scaler = StandardScaler()\n",
        "# scaled_features = scaler.fit_transform(data[numeric_features])\n",
        "\n",
        "# # BERT эмбеддинги\n",
        "# model_name = \"DeepPavlov/rubert-base-cased\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModel.from_pretrained(model_name)\n",
        "# model.eval()\n",
        "\n",
        "# def get_bert_embedding(text):\n",
        "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#     return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "\n",
        "# bert_embeddings = []\n",
        "# for comment in tqdm(data['Comment'], desc=\"BERT embedding\"):\n",
        "#     try:\n",
        "#         emb = get_bert_embedding(comment)\n",
        "#     except Exception:\n",
        "#         emb = np.zeros(768)\n",
        "#     bert_embeddings.append(emb)\n",
        "\n",
        "# bert_features = np.vstack(bert_embeddings)\n",
        "\n",
        "# # Объединение признаков\n",
        "# X = np.hstack([bert_features, scaled_features])\n",
        "\n",
        "# y = data['age_group'].values\n",
        "\n",
        "# # Разделение данных\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Обучение XGBoost\n",
        "# xgb_clf = xgb.XGBClassifier(\n",
        "#     objective=\"multi:softmax\",\n",
        "#     num_class=3,\n",
        "#     max_depth=3,\n",
        "#     learning_rate=0.1,\n",
        "#     n_estimators=100,\n",
        "#     eval_metric=\"mlogloss\",\n",
        "#     random_state=42\n",
        "# )\n",
        "\n",
        "# xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# # Предсказание и оценка\n",
        "# y_pred = xgb_clf.predict(X_test)\n",
        "# print(classification_report(y_test, y_pred))\n",
        "\n",
        "# # Кросс-валидация\n",
        "# cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "# cv_scores = cross_val_score(xgb_clf, X, y, cv=cv, scoring='f1_macro')\n",
        "\n",
        "# print(f'Кросс-валидация F1-score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}')\n",
        "\n",
        "# # Подбор гиперпараметров с помощью GridSearchCV\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# param_grid = {\n",
        "#     'max_depth': [3, 6, 9],\n",
        "#     'learning_rate': [0.01, 0.1, 0.2],\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "# }\n",
        "\n",
        "# grid_search = GridSearchCV(xgb_clf, param_grid, scoring='f1_macro', cv=cv)\n",
        "# grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print(f'Лучшие гиперпараметры: {grid_search.best_params_}')\n",
        "# print(f'Лучший F1-score: {grid_search.best_score_:.4f}')\n",
        "\n",
        "# # Анализ ошибок\n",
        "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# cm = confusion_matrix(y_test, y_pred)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "# disp.plot(cmap='Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rohhZ-wSND2H",
        "outputId": "b0568d67-032b-44c3-b201-bb8392a7d783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spellchecker in /usr/local/lib/python3.11/dist-packages (0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spellchecker) (75.2.0)\n",
            "Requirement already satisfied: inexactsearch in /usr/local/lib/python3.11/dist-packages (from spellchecker) (1.0.2)\n",
            "Requirement already satisfied: soundex>=1.0 in /usr/local/lib/python3.11/dist-packages (from inexactsearch->spellchecker) (1.1.3)\n",
            "Requirement already satisfied: silpa-common>=0.3 in /usr/local/lib/python3.11/dist-packages (from inexactsearch->spellchecker) (0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install spellchecker"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+4svVMu6yMxhJ26jEYD7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}