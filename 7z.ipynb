{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFv6i72P0fvdzrjcb+g1GY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Solo7602/MADPA/blob/main/7z.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLNWYkun1Wc1",
        "outputId": "9aa281f6-cdee-4844-de60-39332e27b345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate BERT embedding...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BERT embedding:  93%|█████████▎| 37153/40000 [1:06:46<03:33, 13.35it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, classification_report\n",
        "\n",
        "\n",
        "# Загрузка данных\n",
        "#Цель обучить модель с помощью Logistic Regresion, Линейсный SVM, Перцептроп\n",
        "data = pd.read_csv('all_dataset_processed.csv').head(40000)\n",
        "\n",
        "# data = data_long.head(2000)\n",
        "\n",
        "numeric_features = ['Spell_Errors', 'Slang_Count', 'Emoji_Count', 'Punctuation_Count', 'Word_Count', 'Avg_Word_Length', 'Comment_Length', 'Ends_With_Punct', 'Has_Slang', 'Has_Spell_Errors']\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(data[numeric_features]).astype(int)\n",
        "\n",
        "# BERT эмбеддинги\n",
        "model_name = \"DeepPavlov/rubert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "\n",
        "bert_file = 'bert_embeddings_40к.npy'\n",
        "\n",
        "if os.path.exists(bert_file):\n",
        "    print(\"Load Saving BERT embedding...\")\n",
        "    bert_features = np.load(bert_file)\n",
        "else:\n",
        "    print(\"Generate BERT embedding...\")\n",
        "    bert_embeddings = []\n",
        "    for comment in tqdm(data['Comment_processed'], desc=\"BERT embedding\"):\n",
        "        try:\n",
        "            emb = get_bert_embedding(comment)\n",
        "        except Exception:\n",
        "            emb = np.zeros(768)\n",
        "        bert_embeddings.append(emb)\n",
        "\n",
        "    bert_features = np.vstack(bert_embeddings)\n",
        "    np.save(bert_file, bert_features)  # Сохраняем в файл\n",
        "\n",
        "\n",
        "# Объединение признаков\n",
        "X = np.hstack([bert_features, scaled_features])\n",
        "\n",
        "y = data['Age_Group'].values\n",
        "\n",
        "# Разделение данных\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "\n",
        "# 1. Логистическая регрессия\n",
        "log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "y_pred_log = log_reg.predict(X_test_scaled)\n",
        "\n",
        "# 2. Линейный SVM\n",
        "linear_svm = LinearSVC(max_iter=100)\n",
        "linear_svm.fit(X_train_scaled, y_train)\n",
        "y_pred_svm = linear_svm.predict(X_test_scaled)\n",
        "\n",
        "# 3. Перцептрон\n",
        "perceptron = Perceptron(max_iter=1000, tol=1e-3)\n",
        "perceptron.fit(X_train_scaled, y_train)\n",
        "y_pred_perc = perceptron.predict(X_test_scaled)\n",
        "\n",
        "# 4. SVM с различными ядрами\n",
        "# Полиномиальное ядро\n",
        "param_grid_poly = {'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'kernel': ['poly']}\n",
        "svm_poly = GridSearchCV(SVC(), param_grid_poly, cv=5, scoring='accuracy')\n",
        "svm_poly.fit(X_train_scaled, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test_scaled)\n",
        "\n",
        "# RBF ядро\n",
        "param_grid_rbf = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 'scale', 'auto'], 'kernel': ['rbf']}\n",
        "svm_rbf = GridSearchCV(SVC(), param_grid_rbf, cv=5, scoring='accuracy')\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "# Визуализация границ решений\n",
        "def plot_decision_boundary(model, X, y, title):\n",
        "    # Берем только первые 2 числовых признака для визуализации\n",
        "    X = X[:, :2].toarray() if hasattr(X, \"toarray\") else X[:, :2]\n",
        "\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    # Создаем фиктивные признаки для остальных 1000 измерений\n",
        "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    dummy_features = np.zeros((mesh_points.shape[0], 1000))\n",
        "    mesh_points = np.hstack([mesh_points, dummy_features])\n",
        "\n",
        "    Z = model.predict(mesh_points)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Paired)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
        "    plt.xlabel('Comment Length (scaled)')\n",
        "    plt.ylabel('Sex_Male (scaled)')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(log_reg, X_train_scaled, y_train, \"Logistic Regression Decision Boundaries\")\n",
        "plot_decision_boundary(svm_rbf.best_estimator_, X_train_scaled, y_train, \"SVM with RBF Kernel Decision Boundaries\")\n",
        "\n",
        "# ROC-кривые для бинарной классификации\n",
        "y_binary = np.where(y == 2, 1, 0)\n",
        "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "log_reg_bin = LogisticRegression(max_iter=1000)\n",
        "log_reg_bin.fit(X_train_bin, y_train_bin)\n",
        "\n",
        "svm_rbf_bin = SVC(kernel='rbf', probability=True)\n",
        "svm_rbf_bin.fit(X_train_bin, y_train_bin)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "y_prob_log = log_reg_bin.predict_proba(X_test_bin)[:, 1]\n",
        "fpr_log, tpr_log, _ = roc_curve(y_test_bin, y_prob_log)\n",
        "roc_auc_log = roc_auc_score(y_test_bin, y_prob_log)\n",
        "plt.plot(fpr_log, tpr_log, label=f'Logistic Regression (AUC = {roc_auc_log:.2f})')\n",
        "\n",
        "y_prob_svm = svm_rbf_bin.predict_proba(X_test_bin)[:, 1]\n",
        "fpr_svm, tpr_svm, _ = roc_curve(y_test_bin, y_prob_svm)\n",
        "roc_auc_svm = roc_auc_score(y_test_bin, y_prob_svm)\n",
        "plt.plot(fpr_svm, tpr_svm, label=f'SVM RBF (AUC = {roc_auc_svm:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Сравнение моделей\n",
        "models = {\n",
        "    'Logistic Regression': y_pred_log,\n",
        "    'Linear SVM': y_pred_svm,\n",
        "    'Perceptron': y_pred_perc,\n",
        "    'SVM Poly': y_pred_poly,\n",
        "    'SVM RBF': y_pred_rbf\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, y_pred in models.items():\n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'F1-score': f1_score(y_test, y_pred, average='weighted')\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nСравнение моделей:\")\n",
        "print(results_df.sort_values(by='Accuracy', ascending=False))\n",
        "\n",
        "# Матрицы ошибок\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['[0,15]', '[15,40]', '[40,80]'],\n",
        "                yticklabels=['[0,15]', '[15,40]', '[40,80]'])\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Истинный класс')\n",
        "    plt.xlabel('Предсказанный класс')\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(y_test, y_pred_log, \"Logistic Regression Confusion Matrix\")\n",
        "plot_confusion_matrix(y_test, y_pred_rbf, \"SVM RBF Confusion Matrix\")"
      ]
    }
  ]
}